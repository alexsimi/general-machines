---
title: "Untitled"
author: "simi"
date: "2024-08-15"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Helper packages
library(dplyr) # for basic data wrangling
# Modeling packages

library(keras) # for fitting DNNs


```


```{r}
# Import MNIST training data
mnist <- dslabs::read_mnist()
mnist_x <- mnist$train$images
mnist_y <- mnist$train$labels
```

```{r}
# Rename columns and standardize feature values
colnames(mnist_x) <- paste0("V", 1:ncol(mnist_x))
mnist_x <- mnist_x / 255
# One-hot encode response
mnist_y <- to_categorical(mnist_y, 10)
# Get number of features, which weâ€™ll use in our model
p <- ncol(mnist_x)
```


```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 128, input_shape = p) %>%
  layer_dense(units = 64) %>%
  layer_dense(units = 10)

model
```


```{r}
model_1 <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = p) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")

model_1
```
```{r}

model <- keras_model_sequential() %>%
# Network architecture
layer_dense(units = 128, activation = "relu", input_shape = p) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax") %>%

# Backpropagation
compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)

```



```{r}

# Train the model
fit1 <- model %>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 25,
batch_size = 128,
validation_split = 0.2,
verbose = FALSE
)

fit1

plot(fit1)


```



```{r}
plot(fit1)

```
##Model Tuning

#Batch Normalization

```{r}

model_w_norm <- keras_model_sequential() %>%

  # Network architecture with batch normalization
layer_dense(units = 256, activation = "relu", input_shape = p) %>%
layer_batch_normalization() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 10, activation = "softmax") %>%

  # Backpropagation
compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)

```



```{r}
# Train the model
fit2<- model_w_norm %>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 25,
batch_size = 128,
validation_split = 0.2,
verbose = FALSE
)

fit2

```




```{r}
plot(fit2)
```


#Regularization
```{r}

model_w_reg <- keras_model_sequential() %>%
# Network architecture with L1 regularization and batch normalization
  
layer_dense(units = 256, activation = "relu", input_shape = p,
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_batch_normalization() %>%
layer_dense(units = 128, activation = "relu",
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_batch_normalization() %>%
layer_dense(units = 64, activation = "relu",
kernel_regularizer = regularizer_l2(0.001)) %>%

layer_batch_normalization() %>%
layer_dense(units = 10, activation = "softmax") %>%

  # Backpropagation
compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)

```




```{r}
# Train the model
fit3<- model_w_reg%>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 25,
batch_size = 128,
validation_split = 0.2,
verbose = FALSE
)

fit3
```



```{r}
plot(fit3)
```



```{r}

model_w_drop <- keras_model_sequential() %>%
# Network architecture with 20% dropout
layer_dense(units = 256, activation = "relu", input_shape = p) %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax") %>%

  # Backpropagation
compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)%>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 25,
batch_size = 128,
validation_split = 0.2,
verbose = FALSE
)

model_w_drop


```

```{r}
plot(model_w_drop)
```



```{r}
# Optimal
min(model_w_drop$metrics$val_loss)

```


```{r}
## [1] 0.0696
max(model_w_drop$metrics$val_acc)
```
#optimizers include:RMSProp,Adam,Adagrad
```{r}
model_w_adj_lrn <- keras_model_sequential() %>%
  
layer_dense(units = 256, activation = "relu", input_shape = p) %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax") %>%
compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c('accuracy')
) %>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 35,
batch_size = 128,
validation_split = 0.2,
callbacks = list(
callback_early_stopping(patience = 5),
callback_reduce_lr_on_plateau(factor = 0.05)
),
verbose = FALSE
)

model_w_adj_lrn


```


```{r}
# Optimal
min(model_w_adj_lrn$metrics$val_loss)

```


```{r}
max(model_w_adj_lrn$metrics$val_acc)
```


```{r}

# Learning rate
plot(model_w_adj_lrn)

```
##Grid search

```{r}
library(tfruns)

FLAGS <- flags(
# Nodes
flag_numeric("nodes1", 256),
flag_numeric("nodes2", 128),
flag_numeric("nodes3", 64),

# Dropout
flag_numeric("dropout1", 0.4),
flag_numeric("dropout2", 0.3),
flag_numeric("dropout3", 0.2),

# Learning paramaters
flag_string("optimizer", "rmsprop"),
flag_numeric("lr_annealing", 0.1)
)

```

#incorporate the flag parameters into the model
```{r}

model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes1, activation = "relu", input_shape =
p) %>%
layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout1) %>%
layer_dense(units = FLAGS$nodes2, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout2) %>%
layer_dense(units = FLAGS$nodes3, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout3) %>%
layer_dense(units = 10, activation = "softmax") %>%
compile(
loss = 'categorical_crossentropy',
metrics = c('accuracy'),
optimizer = FLAGS$optimizer
) %>%
fit(
x = mnist_x,
y = mnist_y,
epochs = 35,
batch_size = 128,
validation_split = 0.2,
callbacks = list(
callback_early_stopping(patience = 5),
callback_reduce_lr_on_plateau(factor = FLAGS$lr_annealing)
),
verbose = FALSE
)

model
```
#To execute the grid search we use tfruns::tuning_run().
```{r}

# Run various combinations of dropout1 and dropout2
runs <- tuning_run("scripts/mnist-grid-search.R",
flags = list(
nodes1 = c(64, 128, 256),
nodes2 = c(64, 128, 256),
nodes3 = c(64, 128, 256),
dropout1 = c(0.2, 0.3, 0.4),
dropout2 = c(0.2, 0.3, 0.4),
dropout3 = c(0.2, 0.3, 0.4),
optimizer = c("rmsprop", "adam"),
lr_annealing = c(0.1, 0.05)
),
sample = 0.05
)
runs %>%
filter(metric_val_loss == min(metric_val_loss)) %>%
glimpse()
```


```{r}


```